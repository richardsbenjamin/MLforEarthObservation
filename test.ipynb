{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552e36e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/local/c24cavig/MLforEarthObservation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.data_loader import (\n",
    "    read_images\n",
    ")\n",
    "from training.train import train_model\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19abc045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image NDBI_20m.img - Width: 150, Height: 150\n",
      "Image LST_100m.img - Width: 30, Height: 30\n",
      "Image NDBI_100m.img - Width: 30, Height: 30\n"
     ]
    }
   ],
   "source": [
    "path_index_h = \"data/NDBI_20m.img\"\n",
    "path_temperature_c = \"data/LST_100m.img\"\n",
    "path_index_c = \"data/NDBI_100m.img\"\n",
    "\n",
    "I_H, cols_h, rows_h, crs_h, transform_h = read_images(path_index_h)\n",
    "print(f\"Image NDBI_20m.img - Width: {cols_h}, Height: {rows_h}\")\n",
    "\n",
    "T_C, cols_t, rows_t, crs_t, transform_t = read_images(path_temperature_c)\n",
    "print(f\"Image LST_100m.img - Width: {cols_t}, Height: {rows_t}\")\n",
    "\n",
    "I_C, cols_c, rows_c, crs_c, transform_c = read_images(path_index_c)\n",
    "print(f\"Image NDBI_100m.img - Width: {cols_c}, Height: {rows_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1dbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sIc=np.std(I_C)\n",
    "sTc=np.std(T_C)\n",
    "mIc=np.mean(I_C)\n",
    "mTc=np.mean(T_C)\n",
    "I_C=(I_C-np.mean(I_C))/np.std(I_C)\n",
    "T_C=(T_C-np.mean(T_C))/np.std(T_C)\n",
    "\n",
    "#Create dataloader\n",
    "signals_all = np.stack((I_C.flatten(), T_C.flatten()),axis=1)\n",
    "signals_all = signals_all[:,:,np.newaxis]\n",
    "\n",
    "class TrajDataSet(Dataset):\n",
    "    def __init__(self,  traj, transform=None):\n",
    "        self.traj = traj\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.traj.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # select coordinates\n",
    "        sample = self.traj[idx,:,:]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        if(cuda):\n",
    "            return torch.FloatTensor(sample).cuda()\n",
    "        else:\n",
    "            return torch.FloatTensor(sample)\n",
    "        #return torch.FloatTensor(sample)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 128\n",
    "batches=signals_all.shape[0]/batch_size\n",
    "\n",
    "## reduce size dataset\n",
    "train_set = TrajDataSet(signals_all, transform= ToTensor())\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers = 0, shuffle = True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1990e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 5, 5, -1]' is invalid for input of size 150528",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model2, input_stats, target_stats = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI_C\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_C\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m torch.save(model2.state_dict(), \u001b[33m\"\u001b[39m\u001b[33mtraining/swin_transformer_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/users/local/c24cavig/MLforEarthObservation/training/train.py:32\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(input_data, target_data, epochs, batch_size, learning_rate)\u001b[39m\n\u001b[32m     29\u001b[39m targets = batch[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     31\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m loss = criterion(outputs, targets)\n\u001b[32m     34\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/users/local/c24cavig/MLforEarthObservation/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/users/local/c24cavig/MLforEarthObservation/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/users/local/c24cavig/MLforEarthObservation/model/swin_transformer.py:38\u001b[39m, in \u001b[36mSwinTransformerV1.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     36\u001b[39m batch_size = x.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     37\u001b[39m h = w = \u001b[38;5;28mint\u001b[39m(features.shape[\u001b[32m1\u001b[39m] ** \u001b[32m0.5\u001b[39m)  \u001b[38;5;66;03m# Assume features são quadradas\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m features = \u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.permute(\u001b[32m0\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Aplica convolução final\u001b[39;00m\n\u001b[32m     41\u001b[39m output = \u001b[38;5;28mself\u001b[39m.final_conv(features)\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[1, 5, 5, -1]' is invalid for input of size 150528"
     ]
    }
   ],
   "source": [
    "model2, input_stats, target_stats = train_model(I_C, T_C, epochs=50, batch_size=32)\n",
    "\n",
    "torch.save(model2.state_dict(), \"training/swin_transformer_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bdd3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.swin_transformer import SwinTransformerV1\n",
    "from model.data_loader import prepare_dataloader\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52fcf919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "train_loader, input_stats, target_stats = prepare_dataloader(I_C, T_C, 32)\n",
    "\n",
    "# Cria o modelo simplificado\n",
    "model = SwinTransformerV1(\n",
    "    img_size=224,\n",
    "    in_chans=1,\n",
    "    out_chans=1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55de71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    inputs = batch['input'].to(device)\n",
    "    targets = batch['target'].to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6635421",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 5, 5, -1]' is invalid for input of size 150528",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m batch_size = x.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     11\u001b[39m h = w = \u001b[38;5;28mint\u001b[39m(features.shape[\u001b[32m1\u001b[39m] ** \u001b[32m0.5\u001b[39m)  \u001b[38;5;66;03m# Assume features são quadradas\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m features = \u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.permute(\u001b[32m0\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Aplica convolução final\u001b[39;00m\n\u001b[32m     15\u001b[39m output = model.final_conv(features)\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[1, 5, 5, -1]' is invalid for input of size 150528"
     ]
    }
   ],
   "source": [
    "x = inputs\n",
    "\n",
    "if x.dim() == 3:\n",
    "    x = x.unsqueeze(1)\n",
    "    \n",
    "# Extrai features com Swin Transformer\n",
    "features = model.swin.forward_features(x)\n",
    "\n",
    "# Redimensiona as features para formato de imagem\n",
    "batch_size = x.shape[0]\n",
    "h = w = int(features.shape[1] ** 0.5)  # Assume features são quadradas\n",
    "features = features.view(batch_size, h, w, -1).permute(0, 4, 1, 2)\n",
    "\n",
    "# Aplica convolução final\n",
    "output = model.final_conv(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c972f1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28, 192])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
